{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarefa 02\n",
    "\n",
    "1. Cite 5 diferenças entre o AdaBoost e o GBM.\n",
    "2. Acesse o link [Scikit-learn – GBM]('https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html'),  leia a explicação \n",
    "(traduza se for preciso) e crie um jupyter notebook contendo o exemplo de classificação e de regressão do GBM.\n",
    "3. Cite 5 Hyperparametros importantes no GBM.\n",
    "4. (Opcional) Utilize o GridSearch para encontrar os melhores hyperparametros para o conjunto de dados do exemplo.\n",
    "5. Acessando o artigo do Jerome Friedman [(Stochastic)]('https://jerryfriedman.su.domains/ftp/stobst.pdf') e pensando no nome dado ao Stochastic GBM, qual é a maior diferença entre os dois algoritmos?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cite 5 diferenças entre o AdaBoost e o GBM.\n",
    "\n",
    "1. **Loss Function**: O GBM é mais robusto para valores discrepantes (outliers) do que o AdaBoost.\n",
    "2. **GBM**: floresta de árvores | **AdaBoost**: floresta de stumps\n",
    "3. **GBM**: primeiro passo é a média do Y | **AdaBoost**: primeiro passo é um stump\n",
    "4. **GBM**: todas as respostas das árvores possui um multiplicador em comum chamado learning_rate (eta) | **AdaBoost**: cada resposta tem um peso diferente\n",
    "5. **GBM**: All the learners have equal weights in the case of gradient boosting. The weight is usually set as the learning rate which is small in magnitude. | **AdaBoost**: The final prediction is based on a majority vote of the weak learners’ predictions weighted by their individual accuracy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 2. Acesse o link [Scikit-learn – GBM]('https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html'), leia a explicação (traduza se for preciso) e crie um jupyter notebook contendo o exemplo de classificação e de regressão do GBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.009154859960321"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]\n",
    "est = GradientBoostingRegressor(\n",
    "    n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0,\n",
    "    loss='squared_error'\n",
    ").fit(X_train, y_train)\n",
    "mean_squared_error(y_test, est.predict(X_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Cite 5 Hyperparametros importantes no GBM.\n",
    "\n",
    "1. loss\n",
    "2. learning_rate\n",
    "3. n_estimators\n",
    "4. subsample\n",
    "5. criterion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. (Opcional) Utilize o GridSearch para encontrar os melhores hyperparametros para o conjunto de dados do exemplo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Acessando o artigo do Jerome Friedman [(Stochastic)]('https://jerryfriedman.su.domains/ftp/stobst.pdf') e pensando no nome dado ao Stochastic GBM, qual é a maior diferença entre os dois algoritmos?\n",
    "\n",
    "Um grande insight sobre bagging ensembles e random forest foi permitir que árvores fossem criadas de forma gananciosa/generosas a partir de subamostras do conjunto de dados de treinamento.\n",
    "\n",
    "Esse mesmo benefício pode ser usado para reduzir a correlação entre as árvores na sequência em GBM.\n",
    "\n",
    "Essa variação de boosting é chamada de stochastic gradient boosting.\n",
    "\n",
    ">em cada iteração, uma subamostra dos dados de treinamento é extraída aleatoriamente (sem substituição) do conjunto de dados de treinamento completo. A subamostra selecionada aleatoriamente é então usada, em vez da amostra completa, para treinar (fit) a base de aprendizagem.<br><br>\n",
    "— Stochastic Gradient Boosting, 1999"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('modulo-23-Combinacao-de-modelos-1-TvWzh9x3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Índice",
   "title_sidebar": "Conteúdo",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "7940a1f1a323e82d8431485786ac794b7107dfaa1129e0a7c56f20d0ff01539a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
